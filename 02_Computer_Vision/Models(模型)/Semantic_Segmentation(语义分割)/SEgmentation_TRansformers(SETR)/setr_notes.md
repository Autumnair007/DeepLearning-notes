#  SETR笔记

参考资料：[(2 封私信 / 28 条消息) 重新思考语义分割范式——SETR - 知乎](https://zhuanlan.zhihu.com/p/348418189)

------

## SETR模型详解：从序列到序列的视角重构语义分割
在计算机视觉领域，语义分割是一项基本而关键的任务，其目标是为图像中的每一个像素分配一个类别标签。长期以来，以全卷积网络（FCN）为代表的编码器-解码器架构一直是该领域的主流范式。然而，2021年提出的SETR（SEgmentation TRansformer）模型，以其全新的视角彻底改变了这一传统。
SETR的核心思想是**将语义分割重新定义为一个序列到序列（Sequence-to-Sequence）的预测任务**，并首次成功地将纯粹的Transformer架构引入该领域。它摒弃了传统FCN编码器中通过堆叠卷积和池化层来逐步下采样、增大感受野的方式，转而利用Transformer强大的全局上下文建模能力，为语义分割带来了突破性的性能。
本文将详细剖析SETR模型的结构，重点讲解其如何将图像转化为序列，Transformer编码器的工作原理，以及其创新的解码器设计。

### 一、 整体架构：序列到序列的分割范式
SETR模型遵循一个经典的编码器-解码器（Encoder-Decoder）结构，但其内部组件与传统CNN模型截然不同。
1.  **编码器（Encoder）**: 采用一个纯粹的、标准的Transformer编码器。其职责不是像CNN那样生成多尺度、分辨率逐渐降低的特征图，而是将一幅图像编码成一个包含丰富上下文信息的特征序列。
2.  **解码器（Decoder）**: 负责将编码器输出的特征序列上采样，恢复至原始图像的分辨率，并最终在每个像素点上进行分类预测。SETR论文中提出了三种不同复杂度的解码器设计，以适应不同的需求。
*图1: SETR模型整体架构示意图*
### 二、 SETR核心之：编码器
SETR的编码器是其最具革命性的部分，它直接借鉴了自然语言处理（NLP）领域中Transformer的成功经验，特别是受到了Vision Transformer (ViT) 的启发。其工作流程主要包括两个关键步骤：图像序列化和Transformer特征提取。
#### 1. 图像序列化（Image as a Sequence）
Transformer的输入是1D的序列数据（例如句子中的单词序列），而图像是2D的空间数据。为了弥合这一差异，SETR首先需要将输入的2D图像 $x \in \mathbb{R}^{H \times W \times C}$（其中H、W、C分别为高、宽和通道数）转化为一个1D的向量序列。
该过程如下：
  * **图像分块 (Image Patching)**：将图像分割成$N$个固定大小的、不重叠的图像块（Patches）。假设每个图像块的大小为 $P \times P$，那么分割出的图像块数量为：
$$
L = \frac{H \times W}{P^2}
$$
例如，一张 $480 \times 480$ 的图像，如果patch大小为 $16 \times 16$，那么将会得到 $(480 \times 480) / (16 \times 16) = 900$ 个图像块。

  * **展平与线性嵌入 (Flatten & Linear Projection)**：将每个 $P \times P \times C$ 的图像块展平（Flatten）为一个一维向量，其维度为 $P^2C$。接着，通过一个可学习的线性投射层（一个全连接层），将这个高维向量映射到一个固定维度的嵌入向量 $z_i \in \mathbb{R}^{D}$。这个过程可以用以下公式表示：
$$
\mathbf{z}_i = \mathbf{E} \mathbf{x}_p^i
$$
其中，$\mathbf{x}_p^i$ 是第 $i$ 个展平后的图像块向量，$\mathbf{E}$ 是可学习的嵌入矩阵（即线性投射层的权重）。$D$ 是整个Transformer模型的工作维度（Hidden Size）。

  * **位置编码 (Position Embedding)**：标准的Transformer对序列顺序不敏感，因为它同时处理所有输入。然而，图像中各个块的空间位置关系至关重要。为了引入这种空间信息，SETR为每个嵌入向量添加了一个可学习的**位置编码向量** $\mathbf{p}_i \in \mathbb{R}^{D}$。
$$
\mathbf{z'}_0 = [\mathbf{z}_1 + \mathbf{p}_1, \mathbf{z}_2 + \mathbf{p}_2, \dots, \mathbf{z}_L + \mathbf{p}_L]
$$
最终，编码器的输入是一个维度为 $L \times D$ 的嵌入矩阵 $\mathbf{z'}_0$。

#### 2. Transformer编码器层
获得了序列化的图像表示后，SETR将其送入一个由 $M$ 个相同的层堆叠而成的Transformer编码器。每一层主要由两个核心子模块构成：多头自注意力（Multi-Head Self-Attention, MHSA）和前馈网络（Feed-Forward Network, FFN）。
  * **多头自注意力机制 (MHSA)**：这是Transformer的核心，它允许模型在序列中的任意两个位置之间直接建立联系，从而捕获全局依赖关系。对于输入序列中的每一个patch向量，自注意力机制会计算它与序列中所有其他patch向量的关联程度（即“注意力分数”），并以此为权重，对所有patch向量进行加权求和，生成新的表示。
    单个注意力头的计算可以概括为以下公式：
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$
  - $Q$ (Query), $K$ (Key), $V$ (Value) 是从输入嵌入序列 $\mathbf{z'}$ 经过不同的线性变换得到的三个矩阵。
  - $QK^T$ 计算了每个Query与所有Key之间的相似度分数。
  - $\sqrt{d_k}$ 是一个缩放因子，用于稳定梯度，其中 $d_k$ 是Key向量的维度。
  - Softmax函数将分数归一化为概率分布。
  - 最后将该概率与 $V$ (Value)矩阵相乘，得到加权后的输出。
“多头”则意味着并行地执行多次上述计算，每一“头”学习不同的注意力模式，然后将所有头的结果拼接并再次进行线性变换，从而增强了模型捕捉不同类型相关性的能力。MHSA使得编码器的**每一层都拥有全局的感受野**，这是其相对于CNN编码器的最大优势。

  * **前馈网络 (FFN)**：在MHSA之后，每个位置的输出都会经过一个简单的前馈神经网络，通常由两个线性层和一个ReLU或GELU激活函数组成。
$$
\text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2
$$
此外，每个子模块（MHSA和FFN）的输出都采用了残差连接（Residual Connection）和层归一化（Layer Normalization），这对于训练深度Transformer模型至关重要。

经过 $M$ 层的处理后，编码器最终输出一个特征序列 $\mathbf{z}_M \in \mathbb{R}^{L \times D}$。这个序列的维度与输入序列完全相同（$L \times D$），但其中每个向量都已深度融合了图像的全局上下文信息。
### 三、 解码器设计：从序列恢复图像分割图
编码器的输出是一个一维序列，而语义分割需要一个与原图大小相同的二维分割图。SETR的解码器负责实现这一从序列到图像的转换。论文中探索了三种不同的解码器设计。
#### 1. 朴素解码器 (Naive Decoder / SETR-Naive)
这是最简洁的一种设计。它首先将编码器输出的 $L \times D$ 序列特征重塑（Reshape）为二维的特征图，其尺寸为 $\frac{H}{P} \times \frac{W}{P} \times D$。然后，通过一个简单的卷积网络进行处理：
1.  使用一个 $1 \times 1$ 卷积，将通道数 $D$ 映射到语义类别的数量 $N_{cls}$。
2.  直接使用一个**双线性插值（Bilinear Interpolation）**，将特征图一次性上采样 $P$ 倍，恢复到原始图像尺寸 $H \times W \times N_{cls}$。
这种方法的优点是简单高效，但由于一次性上采样倍数较大，可能会损失一些细节信息，导致分割边界不够精细。
#### 2. 渐进式上采样解码器 (Progressive Upsampling / SETR-PUP)
为了解决朴素解码器可能存在的细节损失问题，SETR-PUP采用了一种更平滑的渐进式上采样策略。它不再一步到位，而是通过多个阶段逐步恢复分辨率。
其结构通常是交替使用卷积层和上采样层：
1.  首先将 $L \times D$ 的序列特征重塑为 $\frac{H}{P} \times \frac{W}{P} \times D$ 的特征图。
2.  **阶段1**: 应用一个卷积层（例如 $3 \times 3$ Conv）进行特征融合，然后进行一次 $2 \times$ 的双线性上采样。
3.  **阶段2**: 再次应用一个卷积层，再进行一次 $2 \times$ 的上采样。
4.  重复这个过程，直到特征图的分辨率恢复到 $H \times W$。
例如，对于 $P=16$ 的patch大小，可以设计4个阶段，每个阶段都进行 $2 \times$ 上采样（$16 = 2^4$）。这种循序渐进的方式使得模型能够更好地学习和恢复图像的精细结构。
#### 3. 多级特征聚合解码器 (Multi-Level Feature Aggregation / SETR-MLA)
这是三种设计中最复杂也最有效的一种。它借鉴了特征金字塔网络（FPN）的思想，即融合来自编码器不同层级的特征。然而，与CNN不同，Transformer编码器的每一层输出的序列长度和特征维度都是相同的。
SETR-MLA的设计如下：
1.  从Transformer编码器的 $M$ 层中，均匀地选择 $k$ 个层（例如，第$M/k, 2M/k, \dots, M$层）的输出序列 $\mathbf{z}_i \in \mathbb{R}^{L \times D}$。
2.  对每一个选出的特征序列 $\mathbf{z}_i$，都将其重塑为二维特征图 $\mathbf{f}_i \in \mathbb{R}^{\frac{H}{P} \times \frac{W}{P} \times D}$。
3.  对每个特征图 $\mathbf{f}_i$ 应用一个 $1 \times 1$ 卷积，将其通道数从 $D$ 统一到一个较小的维度 $d$。
4.  将所有经过通道压缩的特征图 $\mathbf{f'}_i \in \mathbb{R}^{\frac{H}{P} \times \frac{W}{P} \times d}$ 在通道维度上进行拼接（Concatenate），形成一个聚合特征图 $\mathbf{f}_{agg} \in \mathbb{R}^{\frac{H}{P} \times \frac{W}{P} \times (k \times d)}$。
5.  最后，将这个聚合后的特征图送入一个类似于SETR-PUP的渐进式上采样模块，或者一个简单的分类头，生成最终的分割结果。
通过融合来自不同深度层的特征（既有浅层的细节信息，也有深层的语义信息），SETR-MLA能够产生比前两种解码器更精确的分割结果。
### 四、 模型变体
与Vision Transformer类似，SETR也可以通过调整Transformer编码器的层数（$M$）、隐藏维度（$D$）以及多头注意力的头数来构建不同规模的模型，以平衡性能和计算成本。后续研究中也出现了如SETR-Small, SETR-Medium, SETR-Large等变体，以适应不同的应用场景。
### 总结
SETR模型是语义分割领域的一个里程碑。它成功地证明了，一个不依赖于卷积和下采样操作的、纯粹的Transformer编码器，也能够在该任务上取得顶尖的性能。其核心贡献在于：
1.  **全新的视角**：将语义分割视为序列到序列任务，为该领域开辟了新的研究方向。
2.  **强大的全局建模**：利用自注意力机制，编码器的每一层都能捕获全局上下文，有效克服了传统FCN感受野受限的问题。
3.  **灵活的解码器设计**：提供了从简单到复杂的三种解码器方案，展示了如何将Transformer编码的序列特征有效地转换为像素级的分割图。
SETR的出现，不仅刷新了多个语义分割基准测试的记录，更重要的是，它启发了后续大量基于Transformer的视觉骨干网络和分割模型的发展，深刻地影响了整个计算机视觉领域的格局。
