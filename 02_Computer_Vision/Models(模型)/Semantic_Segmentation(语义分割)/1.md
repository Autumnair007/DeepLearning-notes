| 模型 (Model)  | 发表年份 | 核心思想                                                     | 主要创新点                                                   | Backbone                                             | Decoder                                                      | 关键技术                                                     | 特点                                                         | 个人评价                                                     |
| :------------ | :------- | :----------------------------------------------------------- | :----------------------------------------------------------- | :--------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |
| **FCN**       | 2015     | 将图像分类网络的全连接层替换为卷积层，实现端到端的像素级密集预测，开创了语义分割的新纪元。 | 1. **全卷积化**: 首次提出将分类网络完全卷积化，使其能接受任意尺寸输入并输出与输入同尺寸的热力图。<br>2. **跳跃连接**: 引入跳跃结构(Skip Architecture)融合深层语义信息和浅层表观信息。 | AlexNet, VGG-16, GoogLeNet等经典分类网络。           | 单纯的上采样层(反卷积/双线性插值)与跳跃连接的特征图相加融合。结构相对简单。 | 全卷积、上采样(Upsampling)、反卷积(Deconvolution)、跳跃连接(Skip Connection)。 | 结构简单，是现代语义分割的基石，但分割边界较为粗糙，对小物体不敏感。 | **奠基者**。FCN的思想如同一颗种子，催生了后续几乎所有的分割模型。虽然其本身性能已被超越，但其"全卷积+上采样"的核心框架影响深远，是理解图像分割演进历史绕不开的起点。 |
| **SegFormer** | 2021     | 摒弃复杂设计，通过构建一个分层的Transformer编码器和一个极度轻量级的MLP解码器，实现了一个简单、高效且性能强大的语义分割框架。 | 1. **分层Transformer编码器**: 不依赖位置编码，能输出多尺度的特征金字塔，兼顾高分辨率细节与深层语义。<br>2. **轻量级全MLP解码器**: 仅用线性层高效融合多尺度特征，参数量极少，计算开销低。 | MiT (Mix-Transformer) 系列 (B0-B5)                   | 一个完全由MLP层(1x1卷积)构成的极简解码器。它将多尺度特征统一通道和尺寸后，直接拼接并用MLP输出预测结果。 | 分层Transformer、高效自注意力(Efficient Self-Attention)、混合前馈网络(Mix-FFN)、重叠块嵌入(Overlapped Patch Merging)。 | 结构简洁高效，性能强大。因摒弃了位置编码，对不同分辨率输入的泛化能力和鲁棒性强。在同等计算量下达到SOTA性能。 | **化繁为简的典范**。SegFormer深刻洞察到强大的编码器可以极大简化解码器的设计，其追求极致效率与性能平衡的设计哲学，使其成为Transformer在分割领域的一个里程碑式的工作，证明了简洁架构的巨大潜力。 |
| **DPT**       | 2021     | 首次成功将纯粹的Vision Transformer (ViT)架构作为骨干网络，通过一个精心设计的卷积解码器，高效地解决了密集预测任务。其核心是“用Transformer的全局视野编码，用卷积的局部精细操作解码”。 | 1. **ViT作为密集预测骨干**: 首次证明了标准的、非分层的ViT可作为密集预测任务的强大通用骨干。<br>2. **Reassemble操作**: 设计了将ViT输出的一维Token序列重新组装成二维图像式特征图的关键模块。<br>3. **多尺度特征融合**: 解码器从ViT编码器的多个中间层提取特征，并逐级上采样融合，结合了全局语义与局部细节。 | Vision Transformer (ViT-Base, ViT-Large, ViT-Hybrid) | 一个基于卷积的解码器。它首先通过`Reassemble`操作将不同ViT层的Tokens转换为特征图，然后通过多个`Fusion`模块（包含残差卷积单元和上采样）逐步融合这些多尺度特征，恢复空间分辨率。 | Vision Transformer (ViT)、自注意力(Self-Attention)、Reassemble操作、多尺度特征融合、残差卷积单元(Residual Conv Unit)。 | 拥有强大的全局上下文建模能力。打破了CNN在密集预测领域的传统范式，为ViT在下游任务的应用开辟了新方向。能灵活处理可变尺寸输入。 | **开创者**。DPT成功地将为分类设计的ViT架构“翻译”到了密集预测任务中，其设计的`Reassemble`和`Fusion`模块优雅地解决了Token到像素的转换难题，为后续大量基于ViT的分割/深度估计模型铺平了道路。 |