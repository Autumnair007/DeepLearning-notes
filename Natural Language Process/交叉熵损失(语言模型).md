#  交叉熵损失(语言模型)笔记（DeepSeek，Gemini 2.5 pro生成）

学习资料：

------

# 交叉熵在语言模型中的具体形式

理解交叉熵在语言模型中的具体形式需要从**交叉熵的定义**和**语言模型的任务特性**出发。以下是通过详细推导解释为什么语言模型中的交叉熵损失是 $-\log P(x_t \mid x_{t-1}, \ldots, x_1)$ 的过程。

---

## 1. 交叉熵的原始定义
交叉熵衡量两个概率分布 $p$（真实分布）和 $q$（预测分布）之间的差异，定义为：
$$
H(p, q) = -\sum_{i} p_i \log q_i
$$
其中 $p_i$ 是真实概率，$q_i$ 是模型预测的概率。

---

## 2. 分类任务中的交叉熵
在分类任务中：
- **真实分布 $p$**：通常是 **One-hot 编码**（例如，真实类别为第 $k$ 类时，$p_k = 1$，其余 $p_i = 0$）。
- **预测分布 $q$**：模型输出的概率分布（例如，经过 Softmax 后的概率）。

此时交叉熵简化为：
$$
H(p, q) = -\sum_{i=1}^C p_i \log q_i = -\log q_k
$$
因为只有 $p_k = 1$，其他项均为 0。

---

## 3. 语言模型的本质是一个序列分类任务
语言模型的目标是：**根据历史词序列 $x_1, x_2, \ldots, x_{t-1}$，预测下一个词 $x_t$ 的概率分布**。  
- **输入**：历史词序列 $x_1, x_2, \ldots, x_{t-1}$。
- **输出**：词表中每个词作为下一个词 $x_t$ 的概率 $P(x_t \mid x_{t-1}, \ldots, x_1)$。

这本质上是一个 **多分类任务**，类别数是词表大小 $V$。

---

## 4. 语言模型中的交叉熵推导
对于时间步 $t$：
- **真实分布 $p$**：One-hot 编码，真实词 $x_t$ 对应位置的值为 1，其他为 0。
- **预测分布 $q$**：模型预测的概率分布 $P(x_t \mid x_{t-1}, \ldots, x_1)$。

根据分类任务中的交叉熵公式，时间步 $t$ 的损失为：
$$
H(p, q) = -\log P(x_t \mid x_{t-1}, \ldots, x_1)
$$

**整个序列的损失**是各个时间步损失的平均值：
$$
H = \frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1)
$$

---

## 5. 具体例子：分步计算
假设词表为 `["apple", "banana", "orange"]`，序列为 `["apple", "banana"]`。  
模型的任务是：根据历史词预测下一个词。

### 时间步 1：预测第一个词 $x_1$
- **输入**：无历史词（或起始符）。
- **真实词**：$x_1 = \text{"apple"}$。
- **模型预测概率**：
  - $P(\text{"apple"}) = 0.7$,
  - $P(\text{"banana"}) = 0.2$,
  - $P(\text{"orange"}) = 0.1$。
- **交叉熵损失**：
$$
H_1 = -\log 0.7 \approx 0.3567
$$

### 时间步 2：预测第二个词 $x_2$
- **输入**：历史词 `["apple"]`。
- **真实词**：$x_2 = \text{"banana"}$。
- **模型预测概率**：
  - $P(\text{"apple"}) = 0.1$,
  - $P(\text{"banana"}) = 0.6$,
  - $P(\text{"orange"}) = 0.3$。
- **交叉熵损失**：
$$
H_2 = -\log 0.6 \approx 0.5108
$$

### 整体损失
$$
H = \frac{H_1 + H_2}{2} = \frac{0.3567 + 0.5108}{2} \approx 0.4338
$$

---

## 6. 为什么是负对数概率？
1. **信息论视角**：  
   - 一个事件的“惊异度”（Surprisal）定义为 $-\log P(\text{event})$。  
   - 概率 $P$ 越高，惊异度越小（模型越不意外）。

2. **优化视角**：  
   - 最小化 $-\log P$ 等价于最大化似然概率 $P$（极大似然估计）。

---

## 7. 语言模型交叉熵的特殊性
- **序列依赖性**：每个时间步的条件概率 $P(x_t \mid x_{t-1}, \ldots, x_1)$ 依赖于历史词，模型需捕捉长距离依赖。
- **平均损失**：对不同长度的序列公平比较（否则长序列的总损失天然更大）。

---

## 总结
语言模型中的交叉熵损失 $-\log P(x_t \mid x_{t-1}, \ldots, x_1)$ 的推导过程如下：
1. **任务定义**：语言模型是序列化的多分类任务。
2. **交叉熵简化**：因真实分布是 One-hot 编码，交叉熵退化为负对数概率。
3. **序列平均**：对每个时间步的损失求平均，得到整体损失。

通过这种方式，交叉熵直接衡量了模型预测下一个词的能力，值越小表示模型预测越准确。

------

## 交叉熵损失与负对数似然函数之间的关系推导

这两个表达式在特定情境下表示的是同一个意思，这既源于它们的数学定义，也与它们在机器学习（尤其是序列模型，如语言模型）中的实际应用意义相关。

我们可以通过数学推导和结合实际意义来理解它们的等价性：

1.  **理解 Cross-Entropy (交叉熵)**:
    *   交叉熵 `H(p, q) = - Σᵢ pᵢ log qᵢ` 衡量的是两个概率分布 `p` 和 `q` 之间的差异。
    *   `p` 通常代表数据的真实分布（ground truth）。
    *   `q` 通常代表模型预测的分布。
    *   交叉熵越小，表示模型预测的分布 `q` 与真实分布 `p` 越接近。

2.  **理解 Negative Log-Likelihood (负对数似然)**:
    *   在序列模型中，我们通常的目标是预测序列中的下一个元素 `xₜ`，给定之前的元素 `x₁, ..., xₜ₋₁`。
    *   模型会输出一个概率分布 `P(X | x₁, ..., xₜ₋₁)`，表示词汇表中每个可能的词作为下一个词 `xₜ` 的概率。
    *   假设在训练数据中，实际出现的下一个词是 `xₜ*`（某个具体的词）。
    *   我们希望模型赋予这个真实出现的词 `xₜ*` 尽可能高的概率，也就是最大化 `P(xₜ* | x₁, ..., xₜ₋₁)`。
    *   在实践中，我们通常最小化其负对数似然：`-log P(xₜ* | x₁, ..., xₜ₋₁)`。取对数是为了计算方便（将乘积变为求和，避免数值下溢），取负号是为了将最大化问题转化为最小化问题（因为 `log` 是单调递增函数，`-log` 是单调递减函数）。

3.  **联系两者**:
    *   考虑预测 `xₜ` 这个任务。
    *   **真实分布 `p`**: 在给定历史 `x₁, ..., xₜ₋₁` 后，下一个词的真实分布是什么？在训练数据中，我们已经观测到了实际发生的下一个词 `xₜ*`。所以，真实的分布可以看作是一个 one-hot 向量：对于词汇表中实际出现的词 `xₜ*`，其概率 `pᵢ` 为 1；对于所有其他词，概率 `pᵢ` 为 0。
    *   **模型预测分布 `q`**: 模型计算出的词汇表中每个词 `i` 作为下一个词的概率，即 `qᵢ = P(xₜ = i | x₁, ..., xₜ₋₁)`。
    *   现在，我们将这两个分布代入交叉熵公式：
        `H(p, q) = - Σᵢ pᵢ log qᵢ`
    *   由于 `p` 是 one-hot 分布，只有当 `i` 等于实际观测到的词 `xₜ*` 时 `pᵢ` 才为 1，其他时候都为 0。所以求和项中只有一项非零：
        `H(p, q) = - (p_{xₜ*} * log q_{xₜ*} + Σ_{i ≠ xₜ*} pᵢ * log qᵢ)`
        `H(p, q) = - (1 * log q_{xₜ*} + Σ_{i ≠ xₜ*} 0 * log qᵢ)`
        `H(p, q) = - log q_{xₜ*}`
    *   这里的 `q_{xₜ*}` 正是模型预测的真实下一个词 `xₜ*` 的概率，即 `P(xₜ* | x₁, ..., xₜ₋₁)`。
    *   因此，`H(p, q) = - log P(xₜ* | x₁, ..., xₜ₋₁)`。

**结论**:

在分类问题（包括序列预测中的下一个词预测）中，当我们将真实标签视为一个 one-hot 分布 `p`，并将模型的预测输出视为概率分布 `q` 时，计算这两个分布之间的 **交叉熵**，其结果**在数学上等价于**计算模型赋予真实标签的概率的**负对数似然**。

所以，这不是一个随意的定义，而是交叉熵这个通用概念在特定应用场景（用模型概率拟合观测数据）下的具体表现形式。在训练神经网络进行分类或序列预测时，我们常说使用“交叉熵损失函数”，实际上就是在最小化观测数据的负对数似然，目的是让模型预测的概率分布尽可能地接近数据的真实分布（即，让模型对实际发生的事件赋予更高的概率）。
