#  文本预处理笔记（DeepSeek生成）

学习资料：[8.2. 文本预处理 — 动手学深度学习 2.0.0 documentation](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/text-preprocessing.html)

[8.3. 语言模型和数据集 — 动手学深度学习 2.0.0 documentation](https://zh-v2.d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html#id6)

[NLP BERT GPT等模型中 tokenizer 类别说明详解-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/1865689)

------

### 文本预处理完整流程笔记总结

**文本预处理**是将原始文本转换为模型可处理数字序列的关键步骤，流程包括：1) **获取数据**（下载并提取有效内容）；2) **清洗文本**（统一格式、去除噪声）；3) **词元化**（按单词或字符拆分）；4) **构建词表**（统计词频并建立词元-索引映射）；5) **数字编码**（将词元转为索引序列）。最终输出结构化数字序列和可逆词表，为模型提供标准化输入，其质量直接影响后续任务（如分类、翻译）的性能。处理时需平衡粒度（词/字符）、词表大小与语义保留，并适配具体语言和任务需求。

---

#### **一、核心目标**  
将原始文本数据转换为**结构化数字序列**，为机器学习模型提供可处理的输入。核心流程分为六个阶段，每个阶段解决特定问题。

---

#### **二、处理流程详解**  
##### 1. **数据获取（Data Acquisition）**  
**目标**：获取原始文本并提取有效内容  
- **数据源**：可直接下载的文本文件（如古腾堡计划电子书）  
- **关键操作**：  
  - 通过HTTP请求获取原始文本  
  - 去除头部/尾部元数据（如版权信息、目录）  
  - 提取正文核心内容  
- **挑战**：处理不同格式的文本来源（需定制切片逻辑）

---

##### 2. **文本清洗（Text Cleaning）**  
**目标**：标准化文本格式，去除噪声  
- **常见操作**：  
  - **大小写统一**：全转小写消除大小写差异  
  - **字符过滤**：  
    - 保留字母、基本标点（,.!?）  
    - 移除特殊符号（@#%^等）  
  - **空格处理**：合并连续空格为单个空格  
  - **冗余内容删除**：去除HTML标签（若存在）  
- **技术实现**：正则表达式匹配替换（`re.sub`）  
- **输出**：纯文本字符串（仅包含有效字符和规范空格）

---

##### 3. **词元化（Tokenization）**  
**目标**：将连续文本拆分为基本单元（词元）  
- **两种粒度**：  
  - **单词级（Word-level）**：  
    - 简单方法：空格分割（`text.split()`，适用于英文）  
    - 高级方法：使用分词工具（如NLTK、spaCy）处理连字符、缩写等  
  - **字符级（Character-level）**：  
    - 直接拆分为单个字符列表  
    - 优点：避免未登录词问题；缺点：序列长度增加  
- **选择依据**：  
  - 任务需求（如拼写检查适合字符级）  
  - 语言特性（中文需专用分词工具）

---

##### 4. **词表构建（Vocabulary Construction）**  
**目标**：建立词元与数字索引的映射关系  

- **关键步骤**：  
  1. **词频统计**：使用`Counter`统计所有词元出现次数  
  2. **排序过滤**：  
     - 按频率降序排列  
     - 丢弃低频词（通过`min_freq`阈值控制词表大小）  
  3. **建立映射**：  
     - `token_to_idx`：词元→索引（快速编码）  
     - `idx_to_token`：索引→词元（解码恢复文本）  
  4. **未知词处理**：  
     - 统一映射到`<unk>`标记（索引0）  
     - 覆盖测试集新词和训练集低频词  
- **输出**：包含词元频率信息的双向查找表

---

##### 5. **数字编码（Numerical Encoding）**  
**目标**：将词元序列转换为模型可处理的数字序列  
- **编码过程**：  
  - 遍历词元列表，查询`token_to_idx`字典  
  - 未登录词自动映射到`<unk>`  
- **解码过程**：  
  - 根据`idx_to_token`列表恢复原始词元  
- **特点**：  
  - 完全可逆（无损转换）  
  - 序列长度与原始词元列表一致

---

##### 6. **流程整合（Pipeline Integration）**  
**目标**：封装完整处理流程为可复用模块  

- **关键设计**：  
  - 参数化控制：  
    - 分词模式（单词/字符）  
    - 最小词频阈值  
    - 数据下载链接  
  - 函数式接口：输入URL，输出数字序列和词表对象  
- **优势**：  
  - 一键处理新数据集  
  - 便于批量处理和实验对比

---

#### **三、扩展优化方向**  
1. **多语言支持**：  
   - 中文：使用jieba分词 + 调整清洗逻辑（保留汉字）  
   - 混合语言：统一Unicode编码处理  

2. **高级分词**：  
   - 子词切分（BPE/WordPiece）平衡词表大小与粒度  
   - 依存句法分析保留语义结构  

3. **流式处理**：  
   - 分块读取大文件（避免内存溢出）  
   - 动态更新词表（适用于在线学习场景）  

4. **特殊标记增强**：  
   - 添加`<pad>`（填充）、`<bos>`（序列开始）、`<eos>`（序列结束）标记  
   - 支持序列生成任务（如机器翻译）  

5. **词表压缩**：  
   - 哈希编码处理超大规模词表  
   - 词干提取（stemming）合并变形词  

---

#### **五、常见问题与解决方案**  
| 问题现象     | 可能原因              | 解决方案                     |
| ------------ | --------------------- | ---------------------------- |
| 编码后全是0  | 清洗过度/词频阈值过高 | 检查清洗逻辑，降低`min_freq` |
| 序列长度爆炸 | 字符级分词 + 长文本   | 改用单词级或子词分词         |
| 内存不足     | 单次加载超大文件      | 分块读取 + 增量统计词频      |
| 标点处理混乱 | 清洗正则表达式不完善  | 调整正则模式保留必要符号     |

---

通过系统化的文本预处理，可为后续的文本分类、机器翻译、情感分析等任务奠定高质量数据基础。实际应用中需根据任务需求灵活调整各阶段处理策略。

------

### 六、长序列数据的读取策略

#### 核心挑战与解决思路
长序列数据（如整本《时间机器》）由于长度超出模型单次处理能力，需进行拆分处理。核心策略是将长序列划分为等长的子序列，通过不同的采样方式生成训练用的小批量数据。关键考量在于如何平衡数据的覆盖性（遍历完整序列）和随机性（防止过拟合）。

---

#### 两种数据采样方法详解

**1. 随机采样（Random Sampling）**
- **核心思想**：每个小批量数据由原始序列中任意位置的子序列构成，相邻批次的子序列无位置关联。
- **实现要点**：
  - 随机偏移起始点：`corpus[random.randint(0, num_steps - 1):]`
  - 计算可划分子序列数：`(len(corpus) - 1) // num_steps`（-1为标签预留）
  - 打乱子序列起始索引：`random.shuffle(initial_indices)`
  - 标签构造：每个样本的标签为下一个时间步的词元序列（Y = X右移1位）

- **代码示例**：
  ```python
  def seq_data_iter_random(corpus, batch_size, num_steps):
      corpus = corpus[random.randint(0, num_steps-1):]
      num_subseqs = (len(corpus)-1) // num_steps
      initial_indices = list(range(0, num_subseqs*num_steps, num_steps))
      random.shuffle(initial_indices)
      
      def data(pos): return corpus[pos:pos+num_steps]
      
      for i in range(0, batch_size*(num_subseqs//batch_size), batch_size):
          indices = initial_indices[i:i+batch_size]
          X = [data(j) for j in indices]
          Y = [data(j+1) for j in indices]  # 标签右移1位
          yield torch.tensor(X), torch.tensor(Y)
  ```
- **输出示例**：
  ```
  X: tensor([[13,14,15,16,17], [28,29,30,31,32]]) 
  Y: tensor([[14,15,16,17,18], [29,30,31,32,33]])
  ```
  *说明*：每个批次包含随机位置的5步子序列，标签为后移1步的序列。

---

**2. 顺序分区（Sequential Partitioning）**
- **核心思想**：保持小批量间的子序列顺序，相邻批次在原始序列中连续。
- **实现要点**：
  - 随机选择初始偏移量：`offset = random.randint(0, num_steps)`
  - 数据重塑为二维：`Xs = Xs.reshape(batch_size, -1)`（行=批量，列=时间步）
  - 按时间步窗口滑动：`X = Xs[:, i:i+num_steps]`

- **代码示例**：
  ```python
  def seq_data_iter_sequential(corpus, batch_size, num_steps):
      offset = random.randint(0, num_steps)
      num_tokens = ((len(corpus)-offset-1)//batch_size)*batch_size
      Xs = torch.tensor(corpus[offset:offset+num_tokens])
      Ys = torch.tensor(corpus[offset+1:offset+1+num_tokens])
      Xs, Ys = Xs.reshape(batch_size,-1), Ys.reshape(batch_size,-1)
      
      for i in range(0, Xs.shape[1]//num_steps * num_steps, num_steps):
          yield Xs[:,i:i+num_steps], Ys[:,i:i+num_steps]
  ```
- **输出示例**：
  ```
  X: tensor([[0,1,2,3,4], [17,18,19,20,21]])
  Y: tensor([[1,2,3,4,5], [18,19,20,21,22]])
  ```
  *说明*：每批的两个样本在原始序列中相距17步，内部子序列保持连续。

---

#### 方法对比与选择
| 特性       | 随机采样                 | 顺序分区               |
| ---------- | ------------------------ | ---------------------- |
| 数据连续性 | ❌ 相邻批次无位置关联     | ✅ 保持原始顺序         |
| 覆盖性     | ✅ 通过随机偏移提高多样性 | ❌ 依赖初始偏移量选择   |
| 内存效率   | 需存储所有子序列索引     | 直接重塑矩阵，内存高效 |
| 适用场景   | 通用语言模型训练         | 需保持上下文连贯的任务 |

---

#### 数据加载器封装
通过`SeqDataLoader`类统一接口，根据参数选择采样方式：
```python
class SeqDataLoader:
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        self.data_iter_fn = seq_data_iter_random if use_random_iter else seq_data_iter_sequential
        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps
    
    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)

def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):
    return SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens), vocab
```
- **使用示例**：`data_iter, vocab = load_data_time_machine(32, 5, use_random_iter=True)`

---

#### 关键设计思想
1. **标签构造**：语言模型本质是预测下一时间步，故标签始终为输入序列右移一位。
2. **长度对齐**：通过`(len-1)//num_steps`确保特征-标签对齐，避免长度不匹配。
3. **矩阵重塑**：顺序分区中，将一维序列重塑为`[batch_size, -1]`，便于按列滑动窗口。
4. **随机性控制**：随机采样通过打乱索引实现，顺序分区通过初始偏移量引入部分随机性。
